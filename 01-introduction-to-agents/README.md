# üö® Mission 01: Introduction to Agents

## üïµÔ∏è‚Äç‚ôÇÔ∏è CODENAME: `OPERATION AI AGENT DECODE`

> **‚è±Ô∏è Operation Time Window:** `~30 minutes ‚Äì intel only, no fieldwork required`  

## üéØ Mission Brief

Welcome, Recruit. Before we dive into building agents, you need a firm grasp of the AI concepts that power them. This mission will equip you with foundational knowledge of conversational AI, large language models (LLMs), retrieval-augmented generation (RAG), and the types of agents you can create in Copilot Studio.

## üîé Objectives

In this mission, you‚Äôll learn:

1. What conversational AI is and why it matters  
1. How Large Language Models (LLMs) power chat experiences  
1. What Retrieval-Augmented Generation (RAG) brings to the table  
1. The distinction between conversational agents and autonomous agents  
1. How agents in Copilot Studio leverage these concepts  

Let‚Äôs dive in!

---

## What Is Conversational AI?

Conversational AI refers to any system that can understand, process, and respond to human language ‚Äì whether text or speech ‚Äì in a way that feels natural. Think chatbots in help desks or virtual personal assistants in your favorite apps. Under the hood, most modern conversational AIs rely on Large Language Models (LLMs), which we‚Äôll cover next.

### Why It Matters

- **User Experience:** Conversational interfaces are often more intuitive than clicking through menus.  
- **Scalability:** One agent can handle tens or hundreds of concurrent conversations.  
- **Efficiency:** Instead of building custom rule-based scripts, LLM-powered agents adapt on the fly to user input.  
- **Extensibility:** With the right design, agents can tap into knowledge bases, connect to APIs, or act as ‚Äúdigital coworkers‚Äù inside business workflows.

---

## Large Language Models (LLMs) 101

At the core of most conversational AI systems are **Large Language Models** ‚Äì neural networks trained on massive text corpora. They learn statistical patterns of language so they can generate coherent sentences, answer questions, or even brainstorm ideas. Key points to understand:

1. **Training Data:** LLMs ingest terabytes of text (web pages, books, articles). This ‚Äúworld knowledge‚Äù lets them respond on many topics.  
1. **Tokenization:** Text is broken into smaller units called tokens (words, subwords, or characters). The model predicts one token at a time.  
1. **Context Window:** Each LLM has a limit on how many tokens it can ‚Äúsee‚Äù at once. Beyond that limit, prior tokens get truncated.  
1. **Prompting:** You interact with an LLM by sending it a prompt. The better your prompt, the more focused and relevant the response.  
1. **Zero-shot vs. Fine-tuning:** Zero-shot means using an LLM as-is (just raw weights). Fine-tuning means adjusting the model on domain-specific data so it answers more accurately for your needs.

> [!TIP]
> A common analogy is that an LLM is like a ‚Äúsuper-smart autocomplete.‚Äù It doesn‚Äôt truly understand meaning like a human brain, but it‚Äôs extremely good at predicting the next best word (or phrase) in a sequence.

---

## Retrieval-Augmented Generation (RAG)

When LLMs rely solely on static training data, they might hallucinate or become outdated. RAG addresses this by letting the model ‚Äúlook up‚Äù fresh information before composing an answer. At a high level, RAG works like this:

1. **User Query:** A user asks a question (e.g., ‚ÄúWhat‚Äôs the latest on Contoso‚Äôs quarterly earnings?‚Äù).  
1. **Retriever Step:** The system queries a knowledge source (documents, internal databases, SharePoint libraries, etc.) to find relevant passages.  
1. **Augmentation:** Retrieved passages get appended to or prepended before the LLM prompt.  
1. **Generation:** The LLM ingests both the user‚Äôs question and the retrieved context, then generates a response that‚Äôs grounded in up-to-date data.  

With RAG, your agent can call internal company wikis, plugin APIs, or search an FAQ knowledge base‚Äîand return answers that aren‚Äôt limited to statically published model parameters.

---

## Conversational vs. Autonomous Agents

In the context of Copilot Studio, the term **agent** can refer to multiple flavors of AI assistants. It‚Äôs helpful to draw a line between:

1. **Conversational Agents:**  
   1. Primarily focus on two-way dialogue.  
   1. Persist context across multiple turns of a conversation.  
   1. Usually orchestrated via predefined flows or triggers (e.g., ‚ÄúIf user says X, respond with Y‚Äù).  
   1. Ideal for customer support, FAQs, guided interactions, scheduling, or simple Q&A.  
   1. Examples:  
      - A Teams chatbot that answers HR policy questions.  
      - A Power Virtual Agents bot on a SharePoint page to guide users through a form.  

1. **Autonomous Agents:**  
   1. Go beyond back-and-forth chat; they can **take actions** on behalf of the user.  
   1. Use LLM reasoning loops (think ‚Äúplan ‚Üí act ‚Üí observe ‚Üí replan‚Äù) to complete tasks.  
   1. Hook into external tools or APIs (e.g., call a Power Automate flow, send calendar invites, manipulate data in Dataverse).  
   1. Operate without constant human prompts‚Äîonce triggered, they can handle multi-step processes autonomously.  
   1. Examples:  
      - An agent that generates a travel itinerary, books flights, and emails confirmations.  
      - A ‚ÄúMeeting Summarizer‚Äù agent that joins a Teams call, transcribes it in real time, and writes an executive summary to OneNote.  

> [!NOTE]
> **Key Difference:** Conversational agents wait for user input and stick to dialogue. Autonomous agents proactively plan and execute a sequence of steps using broader tool access.

---

## Agents in Copilot Studio

**Copilot Studio** unifies both conversational and autonomous scenarios under one framework. Here‚Äôs how Copilot Studio helps you build agents:

1. **Visual Agent Designer:** A low-code canvas to define prompts, memory, and tools for both chat and action workflows.  
1. **LLM Configurations:** Select from various OpenAI models or Microsoft‚Äôs enterprise-grade GPT to match your performance and cost needs.  
1. **Retrieval Connectors:** Prebuilt integrations for SharePoint, OneDrive, Azure Cognitive Search, and Dataverse, enabling RAG out of the box.  
1. **Custom Tools & Functions:** Define custom HTTP actions or Power Automate flows that your agent can invoke autonomously.  
1. **Multi-Modal Support:** Beyond text, Copilot Studio agents can ingest images, files, or structured data to enrich context.  
1. **Publishing & Distribution:** Once your agent is ready, you can publish it to Microsoft 365 Copilot (so users invoke it in Teams, SharePoint, Outlook, etc.) or embed it as a standalone chat widget on a webpage.

---

## üéâ Mission Complete

You‚Äôve now completed your introduction to agents and foundational AI concepts. You understand:

1. **LLMs = The ‚ÄúBrain‚Äù of Your Agent**  
   - Responsible for language understanding and generation.  
   - More tokens = richer context, but also higher cost per call.  

1. **RAG = Real-Time Knowledge Integration**  
   - Bridges the gap between a static LLM and ever-changing data sources.  
   - Retrieves and injects relevant documents or records into the LLM prompt.  

1. **Conversational vs. Autonomous**  
   - **Conversational:** Focus on dialogue flow and preserving context (e.g., ‚ÄúSession Memory‚Äù).  
   - **Autonomous:** Add ‚ÄúAction Blocks‚Äù that let the agent call out to external tools or services.

---
Next up, you'll explore the [fundamentals of Copilot Studio](/02-copilot-studio-fundamentals/README.md)!

Stay sharp, Recruit - your AI journey is just beginning!

## Additional Resources

- **Copilot Studio Documentation Home**  
  <https://learn.microsoft.com/microsoft-copilot-studio/>  

---

![mcs-agent-academy-recruit-01](https://m365-visitor-stats.azurewebsites.net/?resource=https://github.com/microsoft/mcs-agent-academy-recruit/tree/main/01-introduction-to-agents)
